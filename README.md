
---

# PARSIMONY, ORDER & BALANCE â€” Compression Principles for MoE

> Open-source implementation for compressing Mixture-of-Experts (MoE) models via **low-rank decomposition (D)**, **expert pruning (P)**, and **expert merging (M)**, with support for **sequential combinations** and **contribution-ratio allocation**.

* **Paper:** PARSIMONY, ORDER AND BALANCE: PRINCIPLES FOR COMPRESSING MIXTURE-OF-EXPERTS MODELS
* **Authors:** Shuangyou Feng, Chenyu Xu, Yi Ding, Zhi Liang, Sihai Zhang
* **Code:** [https://github.com/sonder0127/compression-MoE](https://github.com/sonder0127/compression-MoE)

<!-- ===== Figures at the beginning ===== -->
![Figure 1 â€” Overview of compression strategies and sequential compositions](assets/Methodlogy.png)
*Figure 1. Overview of our MoE compression setting and sequential combinations (D/P/M) with contribution allocations.*

![Figure 8 â€” Best strategy by compression level and method mix](assets/Conclusion_heatmap.png)
*Figure 8. Best-performing strategy across compression levels. Balanced contributions tend to win under higher compression.*

## ğŸ” TL;DR

* æˆ‘ä»¬ç³»ç»ŸåŒ–è¯„æµ‹äº† MoE å‹ç¼©çš„å•æ³•ã€ä¸¤ä¸¤ç»„åˆä¸ä¸‰æ³•é¡ºåºç»„åˆï¼Œå¹¶åœ¨ä¸åŒ**å‹ç¼©ç‡**ä¸**æ–¹æ³•è´¡çŒ®åˆ†é…**ä¸‹ç»™å‡ºå¯¹æ¯”ä¸åŸåˆ™ã€‚ç»“è®ºé«˜åº¦æ¦‚æ‹¬ä¸ºï¼š**Parsimonyï¼ˆèŠ‚åˆ¶ï¼‰ã€Orderï¼ˆé¡ºåºï¼‰ã€Balanceï¼ˆå‡è¡¡ï¼‰**ã€‚
* **D ä¸ P**åœ¨ä»»æ„é¡ºåºä¸‹**ååŒ**ï¼›**M ä¸ D**å¯¹**é¡ºåºæ•æ„Ÿ**ï¼Œé€šå¸¸ **Mâ†’D ä¼˜äº Dâ†’M**ï¼›**P ä¸ M**å¤šä¸º**ä¸­æ€§æˆ–æ‹®æŠ—**ï¼Œä¸” **Mâ†’P å¾€å¾€æ›´å·®**ã€‚
* ä½å‹ç¼©ç‡ä¸‹å•æ³•å·²è¶³å¤Ÿï¼›é«˜å‹ç¼©ç‡ä¸‹åˆç†çš„ç»„åˆæ›´æœ‰æ•ˆï¼›**å‡è¡¡çš„è´¡çŒ®åˆ†é…**é€šå¸¸å¸¦æ¥æ›´ä¼˜ç»“æœã€‚
* é€Ÿåº¦ä¸æ˜¾å­˜ï¼šæ›´é«˜å‹ç¼©é™ä½å³°å€¼æ˜¾å­˜ï¼›æé«˜**åˆ†è§£è´¡çŒ®**èƒ½ç¨³å®šæå‡æ¨ç†é€Ÿåº¦ï¼›å½“å‰©ä½™ä¸“å®¶æ•°ä» â‰¥ top-k æ—¶ï¼Œçº¯å‰ªææˆ–åˆå¹¶å¯¹é€Ÿåº¦æå‡æœ‰é™ã€‚

---

## ğŸ—‚ï¸ Project Structure

```
compression-MoE/
â”œâ”€ component/
â”‚  â”œâ”€ modeling_compressdeepseek.py   # DeepSeek-MoE compression wrappers
â”‚  â”œâ”€ modeling_compressqwen.py       # Qwen-MoE compression wrappers
â”‚  â”œâ”€ modeling_compressolmoe.py      # OLMoE compression wrappers
â”‚  â”œâ”€ modeling_merging.py            # Expert grouping & merging (dominant/global; LERP/SLERP)
â”‚  â”œâ”€ modeling_pruning.py            # Expert pruning utilities (scoring & mask application)
â”‚  â””â”€ modeling_svdmlp.py             # SVD-based factorization for expert MLP weights
â”œâ”€ configs/
â”‚  â”œâ”€ pruning_merging_svd.json       # Hyperparameter presets for pruning/merging/SVD sweeps
â”‚  â””â”€ ...                            # Additional experiment configs
â”œâ”€ method/
â”‚  â”œâ”€ acc_evaluator.py               # Accuracy evaluator for QA benchmarks
â”‚  â”œâ”€ calcute.py                     # Contribution-ratio calculator (allocation utilities)
â”‚  â”œâ”€ compress_method.py             # Compression method registry & orchestration
â”‚  â”œâ”€ expert_pruning.py              # Implementations of expert-pruning strategies/metrics
â”‚  â””â”€ ppl_evaluator.py               # Perplexity evaluator for language-modeling tasks
â”œâ”€ utils/
â”‚  â”œâ”€ data_utils.py                  # Dataset loading
â”‚  â””â”€ model_utils.py                 # Model loading
â”œâ”€ assets/                           # Figures and static assets for README/docs
â”œâ”€ README.md
â””â”€ pruning_merging_svd.py            # Main CLI entry for joint pruning/merging/SVD pipeline


```

---

## ğŸ“¦ Installation

```bash
# 1) Create env
conda create -n moe-compress python=3.10 -y
conda activate moe-compress

# 2) æœ¬ä»“åº“ä½œä¸ºåŒ…
pip install -r requirements.txt
```

---

## ğŸš€ Quick Start
æ ¹æ®å‹ç¼©éœ€æ±‚ä¿®æ”¹é…ç½®æ–‡ä»¶

`configs/pruning_merging_svd.json`ï¼ˆç¤ºä¾‹ï¼‰ï¼š

```json
{
    "model_name": "OLMoE",
    "model_path": "outfile/OLMoE/model_saved/ASVD_Seed=3.pt",
    "dataset": "wikitext2",
    "seed": 3,
    "DEV": "cpu",
    "step": 0,
    "updating_nsamples": 16,
  
    "eval": {
      "enabled": true,
      "batch_size": 512,
      "nsamples": 500,
      "wiki_batch_size": 16,
      "wiki_nsamples": 0,
      "gen_seq_len": 1024,
      "model_seq_len": 2048
    },
    "svd": {
      "enabled": true,
      "method": "ASVD",
      "mlp_rank": 300,
      "attn_rank": 2048,
      "compress_ratio": 0.1,
      "whitening_nsamples": 32,
      "load_from_file": false,
      "save_model": false
    },
    "pruning": {
      "enabled": true,
      "eval_nsamples": 256,
      "importance_metrics": "activation_frequency",
      "load_from_file": true,
      "strategy": "fixed",
      "compress_ratio": 0.1,
      "pruning_nsamples": 256,
      "save_model": false
    },
    "merging": {
      "enabled": true,
      "eval_nsamples": 32,
      "eval_object": "weight",
      "metrics": "l2",
      "weighting_factor": "activation_frequency",
      "load_from_file": true,
      "save_model": false,
      "num_expert_group": 59,
      "compress_ratio": 0.1
    },
    "other": {
      "build_basenet": false,
      "num_dominate_expert": 8,
      "fine_tune_path": null
    }
  }
```

---

ç„¶årun
```bash
python pruning_merging_svd.py
```

---

## ğŸ§ª Datasets & Tasks

* è¯­è¨€å»ºæ¨¡ï¼š**WikiText-2**, **PTB**
* QAï¼š**OpenBookQA**, **ARC-Easy**, **ARC-Challenge**, **MathQA**
* è¯„æµ‹æŒ‡æ ‡ï¼š**Perplexity / Accuracy**ï¼›**åå tokens/s**ä¸**å³°å€¼æ˜¾å­˜**ã€‚
* ç¯å¢ƒï¼šç¤ºä¾‹åœ¨ **NVIDIA A100** ä¸Šå®Œæˆï¼Œä½ ä¹Ÿå¯ä»¥åœ¨å…¶ä»– GPU ä¸Šå¤ç°ï¼ˆæ‰¹å¤§å°ä¸ç²¾åº¦å¯èƒ½éœ€è°ƒæ•´ï¼‰ã€‚

---

## ğŸ“œ Citation

å¦‚æœä½ è§‰å¾—è¿™ä¸ªä»“åº“å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ï¼š

```bibtex
@inproceedings{feng2025parsimony,
  title     = {Parsimony, Order and Balance: Principles for Compressing Mixture-of-Experts Models},
  author    = {Feng, Shuangyou and Xu, Chenyu and Ding, Yi and Liang, Zhi and Zhang, Sihai},
  booktitle = {Proceedings of ICASSP},
  year      = {2026},
  note      = {Code: \url{https://github.com/sonder0127/compression-MoE}}
}
```

## ğŸ™ Acknowledgements

* åŸºåº§ä¸è·¯ç”±è®¾ç½®å‚è€ƒ OLMoEï¼›æ•°æ®ä¸ä»»åŠ¡æ¥è‡ªå…¬å¼€æ•°æ®é›†ã€‚
